{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Generator using CNN and LSTM\n",
    "\n",
    "This notebook demonstrates how to build an image caption generator using a combination of Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) networks. We'll use the Flickr8k dataset, which contains 8,000 images, each with 5 different captions.\n",
    "\n",
    "## Overview of the Process\n",
    "1. Download and explore the Flickr8k dataset\n",
    "2. Preprocess the images using a pre-trained CNN (VGG16)\n",
    "3. Preprocess the captions (tokenization, vocabulary creation)\n",
    "4. Build the CNN-LSTM model architecture\n",
    "5. Train the model\n",
    "6. Evaluate model performance\n",
    "7. Generate captions for new images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and Explore the Flickr8k Dataset\n",
    "\n",
    "First, let's install the necessary packages and download the dataset from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy pandas matplotlib tensorflow keras pillow nltk tqdm kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Flickr8k dataset from Kaggle\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download the dataset\n",
    "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# List the contents of the dataset directory\n",
    "dataset_path = path\n",
    "print(\"Dataset contents:\")\n",
    "for item in os.listdir(dataset_path):\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Dataset Structure\n",
    "\n",
    "The Flickr8k dataset typically contains:\n",
    "- A directory of images (Flickr8k_Dataset)\n",
    "- Text files with captions (Flickr8k_text)\n",
    "\n",
    "Let's explore the structure and understand the format of the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Define paths to images and captions\n",
    "images_dir = os.path.join(dataset_path, 'Images')\n",
    "captions_file = os.path.join(dataset_path, 'captions.txt')\n",
    "\n",
    "# Read the captions file\n",
    "captions_df = pd.read_csv(captions_file, delimiter=',')\n",
    "print(captions_df.head())\n",
    "\n",
    "# Count the number of images and captions\n",
    "num_images = len(os.listdir(images_dir))\n",
    "num_captions = len(captions_df)\n",
    "print(f\"Number of images: {num_images}\")\n",
    "print(f\"Number of captions: {num_captions}\")\n",
    "print(f\"Average captions per image: {num_captions / num_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few random images with their captions\n",
    "def display_image_with_captions(image_name, captions):\n",
    "    img_path = os.path.join(images_dir, image_name)\n",
    "    img = Image.open(img_path)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Image: {image_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Captions:\")\n",
    "    for i, caption in enumerate(captions, 1):\n",
    "        print(f\"{i}. {caption}\")\n",
    "\n",
    "# Get a random image and its captions\n",
    "random_images = captions_df['image'].unique()\n",
    "random_image = random.choice(random_images)\n",
    "image_captions = captions_df[captions_df['image'] == random_image]['caption'].tolist()\n",
    "\n",
    "display_image_with_captions(random_image, image_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Images and Captions\n",
    "\n",
    "### 2.1 Image Preprocessing\n",
    "\n",
    "We'll use a pre-trained CNN (VGG16) to extract features from the images. This is more efficient than training a CNN from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet data\n",
    "base_model = VGG16(weights='imagenet')\n",
    "# Remove the last layer (classification layer)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "# Function to extract features from an image\n",
    "def extract_features(image_path):\n",
    "    # Load the image with target size (224, 224)\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    # Convert the image to array\n",
    "    img_array = img_to_array(img)\n",
    "    # Expand dimensions to match the model's expected input shape\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    # Preprocess the image (normalize pixel values, etc.)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    # Extract features\n",
    "    features = model.predict(img_array)\n",
    "    return features\n",
    "\n",
    "# Extract features for all images and store them\n",
    "def extract_all_features(images_dir):\n",
    "    features = {}\n",
    "    # Get list of all image files\n",
    "    image_files = os.listdir(images_dir)\n",
    "    \n",
    "    print(f\"Extracting features for {len(image_files)} images...\")\n",
    "    for image_file in tqdm(image_files):\n",
    "        # Extract features for the image\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        image_features = extract_features(image_path)\n",
    "        # Store features using the image filename as key\n",
    "        features[image_file] = image_features\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all images (this may take some time)\n",
    "# Uncomment the line below to run the feature extraction\n",
    "# features = extract_all_features(images_dir)\n",
    "\n",
    "# To save time, we'll extract features for just a few images for demonstration\n",
    "def extract_sample_features(images_dir, num_samples=10):\n",
    "    features = {}\n",
    "    image_files = random.sample(os.listdir(images_dir), num_samples)\n",
    "    \n",
    "    print(f\"Extracting features for {len(image_files)} sample images...\")\n",
    "    for image_file in tqdm(image_files):\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        image_features = extract_features(image_path)\n",
    "        features[image_file] = image_features\n",
    "    \n",
    "    return features, image_files\n",
    "\n",
    "# Extract features for a sample of images\n",
    "sample_features, sample_images = extract_sample_features(images_dir, num_samples=10)\n",
    "\n",
    "# Save the features to a file\n",
    "import pickle\n",
    "with open('sample_features.pkl', 'wb') as f:\n",
    "    pickle.dump(sample_features, f)\n",
    "\n",
    "print(\"Sample features extracted and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Caption Preprocessing\n",
    "\n",
    "Now, let's preprocess the captions by cleaning the text, creating a vocabulary, and preparing the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean captions\n",
    "def clean_caption(caption):\n",
    "    # Convert to lowercase\n",
    "    caption = caption.lower()\n",
    "    # Remove punctuation\n",
    "    caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the caption\n",
    "    tokens = word_tokenize(caption)\n",
    "    # Remove words with numbers\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Join the tokens back into a string\n",
    "    caption = ' '.join(tokens)\n",
    "    return caption\n",
    "\n",
    "# Process all captions\n",
    "def process_captions(captions_df):\n",
    "    # Create a dictionary to store captions for each image\n",
    "    captions_dict = {}\n",
    "    \n",
    "    # Group captions by image\n",
    "    for index, row in captions_df.iterrows():\n",
    "        image_name = row['image']\n",
    "        caption = row['caption']\n",
    "        \n",
    "        # Clean the caption\n",
    "        cleaned_caption = clean_caption(caption)\n",
    "        \n",
    "        # Add start and end tokens\n",
    "        processed_caption = 'startseq ' + cleaned_caption + ' endseq'\n",
    "        \n",
    "        # Add to dictionary\n",
    "        if image_name not in captions_dict:\n",
    "            captions_dict[image_name] = []\n",
    "        captions_dict[image_name].append(processed_caption)\n",
    "    \n",
    "    return captions_dict\n",
    "\n",
    "# Process all captions\n",
    "captions_dict = process_captions(captions_df)\n",
    "\n",
    "# Display a few examples\n",
    "for image_name in list(captions_dict.keys())[:3]:\n",
    "    print(f\"Image: {image_name}\")\n",
    "    for caption in captions_dict[image_name]:\n",
    "        print(f\"  - {caption}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary of all words in captions\n",
    "def create_vocabulary(captions_dict):\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for image_name, captions in captions_dict.items():\n",
    "        for caption in captions:\n",
    "            # Add all words to vocabulary\n",
    "            vocabulary.update(caption.split())\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "# Create vocabulary\n",
    "vocabulary = create_vocabulary(captions_dict)\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Sample words: {list(vocabulary)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings\n",
    "def create_word_mappings(vocabulary):\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    idx_to_word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "# Create mappings\n",
    "word_to_idx, idx_to_word = create_word_mappings(vocabulary)\n",
    "\n",
    "# Save the mappings\n",
    "with open('word_to_idx.pkl', 'wb') as f:\n",
    "    pickle.dump(word_to_idx, f)\n",
    "with open('idx_to_word.pkl', 'wb') as f:\n",
    "    pickle.dump(idx_to_word, f)\n",
    "\n",
    "print(\"Word mappings created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare Training Data\n",
    "\n",
    "Now, let's prepare the data for training the model. We'll create sequences of words for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Function to create sequences for training\n",
    "def create_sequences(captions_dict, features, word_to_idx, max_length):\n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    # Process each image and its captions\n",
    "    for image_name, captions in captions_dict.items():\n",
    "        # Skip images for which we don't have features\n",
    "        if image_name not in features:\n",
    "            continue\n",
    "        \n",
    "        # Get image features\n",
    "        image_features = features[image_name]\n",
    "        \n",
    "        # Process each caption for the image\n",
    "        for caption in captions:\n",
    "            # Convert caption to sequence of word indices\n",
    "            seq = [word_to_idx[word] for word in caption.split() if word in word_to_idx]\n",
    "            \n",
    "            # Create input-output pairs for each word in the caption\n",
    "            for i in range(1, len(seq)):\n",
    "                # Input: image features and sequence up to current word\n",
    "                in_seq = seq[:i]\n",
    "                # Output: next word (target)\n",
    "                out_seq = seq[i]\n",
    "                \n",
    "                # Pad the input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # One-hot encode the output word\n",
    "                out_seq = to_categorical([out_seq], num_classes=len(word_to_idx))[0]\n",
    "                \n",
    "                # Add to training data\n",
    "                X1.append(image_features)\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    \n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "# Find the maximum caption length\n",
    "def get_max_length(captions_dict):\n",
    "    max_length = 0\n",
    "    for image_name, captions in captions_dict.items():\n",
    "        for caption in captions:\n",
    "            length = len(caption.split())\n",
    "            if length > max_length:\n",
    "                max_length = length\n",
    "    return max_length\n",
    "\n",
    "# Get maximum caption length\n",
    "max_length = get_max_length(captions_dict)\n",
    "print(f\"Maximum caption length: {max_length}\")\n",
    "\n",
    "# Create sequences for training (using sample features for demonstration)\n",
    "# Filter captions_dict to only include images in sample_features\n",
    "sample_captions_dict = {image: captions_dict[image] for image in sample_images if image in captions_dict}\n",
    "\n",
    "# Create sequences\n",
    "X1, X2, y = create_sequences(sample_captions_dict, sample_features, word_to_idx, max_length)\n",
    "\n",
    "print(f\"Number of training samples: {len(X1)}\")\n",
    "print(f\"Image features shape: {X1.shape}\")\n",
    "print(f\"Text sequence shape: {X2.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the CNN-LSTM Model\n",
    "\n",
    "Now, let's build the model architecture that combines CNN features with LSTM for caption generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the model architecture\n",
    "def build_model(vocab_size, max_length):\n",
    "    # Image feature input\n",
    "    inputs1 = Input(shape=(4096,))  # Shape of VGG16 features\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "    # Sequence input\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    \n",
    "    # Decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # Combine the inputs and outputs into a single model\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(len(word_to_idx), max_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "Now, let's train the model using the prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data generator for training\n",
    "def data_generator(X1, X2, y, batch_size):\n",
    "    # Get the number of samples\n",
    "    num_samples = len(X1)\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    while True:\n",
    "        # Shuffle indices for each epoch\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Create batches\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            \n",
    "            # Get batch data\n",
    "            batch_X1 = X1[batch_indices]\n",
    "            batch_X2 = X2[batch_indices]\n",
    "            batch_y = y[batch_indices]\n",
    "            \n",
    "            yield [batch_X1, batch_X2], batch_y\n",
    "\n",
    "# Train the model\n",
    "# Note: In a real scenario, you would train on the full dataset\n",
    "# For demonstration, we're using a small sample\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Create a data generator\n",
    "generator = data_generator(X1, X2, y, batch_size)\n",
    "\n",
    "# Train the model\n",
    "steps_per_epoch = len(X1) // batch_size\n",
    "if steps_per_epoch == 0:  # Ensure at least one step per epoch\n",
    "    steps_per_epoch = 1\n",
    "\n",
    "history = model.fit(\n",
    "    generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('image_caption_model.h5')\n",
    "print(\"Model trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model Performance\n",
    "\n",
    "Let's evaluate the model's performance by generating captions for some test images and comparing them with the actual captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a caption for an image\n",
    "def generate_caption(model, image_features, word_to_idx, idx_to_word, max_length):\n",
    "    # Start with the start sequence token\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    # Generate caption word by word\n",
    "    for i in range(max_length):\n",
    "        # Encode the current input sequence\n",
    "        sequence = [word_to_idx[word] for word in in_text.split() if word in word_to_idx]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        \n",
    "        # Predict the next word\n",
    "        yhat = model.predict([image_features, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        \n",
    "        # Map the predicted word index to the actual word\n",
    "        word = idx_to_word[yhat]\n",
    "        \n",
    "        # Stop if we reach the end token\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "        # Append the word to the caption\n",
    "        in_text += ' ' + word\n",
    "    \n",
    "    # Remove start and end tokens from the final caption\n",
    "    final_caption = in_text.replace('startseq', '')\n",
    "    \n",
    "    return final_caption.strip()\n",
    "\n",
    "# Generate captions for sample images\n",
    "for i, image_name in enumerate(sample_images[:3]):\n",
    "    # Get image features\n",
    "    image_features = sample_features[image_name]\n",
    "    \n",
    "    # Generate caption\n",
    "    generated_caption = generate_caption(model, image_features, word_to_idx, idx_to_word, max_length)\n",
    "    \n",
    "    # Display image and captions\n",
    "    img_path = os.path.join(images_dir, image_name)\n",
    "    img = Image.open(img_path)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Image: {image_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Generated Caption:\")\n",
    "    print(generated_caption)\n",
    "    \n",
    "    print(\"\\nActual Captions:\")\n",
    "    if image_name in captions_dict:\n",
    "        for j, caption in enumerate(captions_dict[image_name], 1):\n",
    "            # Remove start and end tokens for display\n",
    "            clean_caption = caption.replace('startseq', '').replace('endseq', '').strip()\n",
    "            print(f\"{j}. {clean_caption}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Caption Generator for New Images\n",
    "\n",
    "Now, let's create a function to generate captions for new images that weren't part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate caption for a new image\n",
    "def caption_image(image_path, model, word_to_idx, idx_to_word, max_length):\n",
    "    # Extract features from the image\n",
    "    image_features = extract_features(image_path)\n",
    "    \n",
    "    # Generate caption\n",
    "    caption = generate_caption(model, image_features, word_to_idx, idx_to_word, max_length)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Test the caption generator on a new image\n",
    "# You can replace this with any image path\n",
    "test_image_path = os.path.join(images_dir, random.choice(os.listdir(images_dir)))\n",
    "\n",
    "# Generate caption\n",
    "caption = caption_image(test_image_path, model, word_to_idx, idx_to_word, max_length)\n",
    "\n",
    "# Display the image and caption\n",
    "img = Image.open(test_image_path)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Test Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Generated Caption:\")\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Implementation\n",
    "\n",
    "Let's create a complete implementation that can be used to generate captions for any image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class ImageCaptionGenerator:\n",
    "    def __init__(self, model_path, word_to_idx_path, idx_to_word_path, max_length):\n",
    "        # Load the model\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        # Load word mappings\n",
    "        with open(word_to_idx_path, 'rb') as f:\n",
    "            self.word_to_idx = pickle.load(f)\n",
    "        with open(idx_to_word_path, 'rb') as f:\n",
    "            self.idx_to_word = pickle.load(f)\n",
    "        \n",
    "        # Set maximum caption length\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load VGG16 model for feature extraction\n",
    "        base_model = VGG16(weights='imagenet')\n",
    "        self.feature_extractor = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "    \n",
    "    def extract_features(self, image_path):\n",
    "        # Load the image\n",
    "        img = load_img(image_path, target_size=(224, 224))\n",
    "        # Convert to array\n",
    "        img_array = img_to_array(img)\n",
    "        # Expand dimensions\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        # Preprocess the image\n",
    "        img_array = preprocess_input(img_array)\n",
    "        # Extract features\n",
    "        features = self.feature_extractor.predict(img_array, verbose=0)\n",
    "        return features\n",
    "    \n",
    "    def generate_caption(self, image_path):\n",
    "        # Extract features\n",
    "        image_features = self.extract_features(image_path)\n",
    "        \n",
    "        # Start with start token\n",
    "        in_text = 'startseq'\n",
    "        \n",
    "        # Generate caption word by word\n",
    "        for i in range(self.max_length):\n",
    "            # Encode the current input sequence\n",
    "            sequence = [self.word_to_idx[word] for word in in_text.split() if word in self.word_to_idx]\n",
    "            sequence = pad_sequences([sequence], maxlen=self.max_length)\n",
    "            \n",
    "            # Predict the next word\n",
    "            yhat = self.model.predict([image_features, sequence], verbose=0)\n",
    "            yhat = np.argmax(yhat)\n",
    "            \n",
    "            # Map the predicted word index to the actual word\n",
    "            word = self.idx_to_word[yhat]\n",
    "            \n",
    "            # Stop if we reach the end token\n",
    "            if word == 'endseq':\n",
    "                break\n",
    "                \n",
    "            # Append the word to the caption\n",
    "            in_text += ' ' + word\n",
    "        \n",
    "        # Remove start token from the final caption\n",
    "        final_caption = in_text.replace('startseq', '')\n",
    "        \n",
    "        return final_caption.strip()\n",
    "\n",
    "# Example usage (uncomment when you have a trained model)\n",
    "'''\n",
    "# Initialize the caption generator\n",
    "caption_generator = ImageCaptionGenerator(\n",
    "    model_path='image_caption_model.h5',\n",
    "    word_to_idx_path='word_to_idx.pkl',\n",
    "    idx_to_word_path='idx_to_word.pkl',\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "# Generate caption for an image\n",
    "image_path = 'path/to/your/image.jpg'\n",
    "caption = caption_generator.generate_caption(image_path)\n",
    "print(f\"Caption: {caption}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've built an image caption generator using a combination of CNN (VGG16) and LSTM networks. The model extracts features from images using a pre-trained CNN and then generates captions using an LSTM network.\n",
    "\n",
    "### What we've accomplished:\n",
    "1. Downloaded and explored the Flickr8k dataset\n",
    "2. Preprocessed images using VGG16 for feature extraction\n",
    "3. Preprocessed captions (cleaning, tokenization, vocabulary creation)\n",
    "4. Built a CNN-LSTM model architecture\n",
    "5. Trained the model on the dataset\n",
    "6. Evaluated the model's performance\n",
    "7. Implemented a caption generator for new images\n",
    "\n",
    "### Possible improvements:\n",
    "1. Use a larger dataset (e.g., Flickr30k, MSCOCO) for better performance\n",
    "2. Try different pre-trained CNN models (e.g., ResNet, Inception)\n",
    "3. Implement attention mechanisms to focus on relevant parts of the image\n",
    "4. Use more advanced NLP techniques (e.g., transformers) for caption generation\n",
    "5. Implement beam search for better caption generation\n",
    "6. Fine-tune the CNN part of the model for better feature extraction\n",
    "\n",
    "### Resources for further learning:\n",
    "1. [Show and Tell: A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555)\n",
    "2. [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)\n",
    "3. [Deep Visual-Semantic Alignments for Generating Image Descriptions](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
